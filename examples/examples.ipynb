{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "departmental-constraint",
   "metadata": {},
   "source": [
    "# torchsynth examples\n",
    "\n",
    "We walk through basic functionality of `torchsynth` in this Jupyter notebook.\n",
    "Just note that all ipd.Audio play widgets normalize the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnotebook(): # pragma: no cover\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        if shell == \"google.colab._shell\":\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interprete\n",
    "    \n",
    "print(f\"isnotebook = {isnotebook()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-worth",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if isnotebook(): # pragma: no cover\n",
    "    import IPython.display as ipd\n",
    "    from IPython.core.display import display\n",
    "    import librosa\n",
    "    import librosa.display\n",
    "    import matplotlib.pyplot as plt\n",
    "else:\n",
    "    class IPD():\n",
    "        def Audio(*args, **kwargs):\n",
    "            pass\n",
    "        def display(*args, **kwargs):\n",
    "            pass\n",
    "    ipd = IPD()\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.fft\n",
    "import torch.tensor as T\n",
    "\n",
    "from torchsynth.module import TorchADSR, TorchSineVCO, TorchVCA, TorchNoise, TorchFmVCO\n",
    "from torchsynth.defaults import SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run examples on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot(signal, sample_rate=SAMPLE_RATE, show=True):\n",
    "    if isnotebook(): # pragma: no cover\n",
    "        t = np.linspace(0, len(signal) / sample_rate, len(signal), endpoint=False)\n",
    "        plt.plot(t, signal)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_plot(signal, sample_rate=SAMPLE_RATE):\n",
    "    if isnotebook(): # pragma: no cover\n",
    "        X = librosa.stft(signal)\n",
    "        Xdb = librosa.amplitude_to_db(abs(X))\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        librosa.display.specshow(Xdb, sr=sample_rate, x_axis=\"time\", y_axis=\"log\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-behalf",
   "metadata": {},
   "source": [
    "## The Envelope\n",
    "Our module is based on an ADSR envelope, standing for \"attack, decay, sustain,\n",
    "release,\" which is specified by four values:\n",
    "\n",
    "- a: the attack time, in seconds; the time it takes for the signal to ramp\n",
    "     from 0 to 1.\n",
    "- d: the decay time, in seconds; the time to 'decay' from a peak of 1 to a\n",
    "     sustain level.\n",
    "- s: the sustain level; a value between 0 and 1 that the envelope holds during\n",
    "a sustained note (**not a time value**).\n",
    "- r: the release time, in seconds; the time it takes the signal to decay from\n",
    "     the sustain value to 0.\n",
    "\n",
    "Envelopes are used to modulate a variety of signals; usually one of pitch,\n",
    "amplitude, or filter cutoff frequency. In this notebook we will use the same\n",
    "envelope to modulate several different audio parameters.\n",
    "\n",
    "### A note about note-on, note-off behaviour \n",
    "\n",
    "By default, this envelope reacts as if it was triggered with midi, for example\n",
    "playing a keyboard. Each midi event has a beginning and end: note-on, when you\n",
    "press the key down; and note-off, when you release the key. `note_on_duration`\n",
    "is the amount of time that the key is depressed. During the note-on, the\n",
    "envelope moves through the attack and decay sections of the envelope. This\n",
    "leads to musically-intuitive, but programatically-counterintuitive behaviour.\n",
    "\n",
    "Assume attack is 0.5 seconds, and decay is 0.5 seconds. If a note is held for\n",
    "0.75 seconds, the envelope won't traverse through the entire attack-and-decay\n",
    "phase (specifically, it will execute the entire attack, and 0.25 seconds of\n",
    "the decay).\n",
    "\n",
    "If this is confusing, don't worry about it. ADSR's do a lot of work behind the\n",
    "scenes to make the playing experience feel natural. Alternately, you may\n",
    "specify one-shot mode (see below), which is more typical of drum machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis parameters.\n",
    "a = T([0.1, 0.2])\n",
    "d = T([0.1, 0.2])\n",
    "s = T([0.75, 0.8])\n",
    "r = T([0.5, 0.8])\n",
    "alpha = T([3.0, 4.0])\n",
    "note_on_duration = T([0.5, 1.5])\n",
    "\n",
    "# Envelope test\n",
    "adsr = TorchADSR(T(a), T(d), T(s), T(r), T(alpha)).to(device)\n",
    "envelope = adsr.forward1D(T(note_on_duration))\n",
    "time_plot(envelope.clone().detach().cpu().T, adsr.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-cable",
   "metadata": {},
   "source": [
    "Here's the l1 error between the two envelopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = torch.mean(torch.abs(envelope[0, :] - envelope[1, :]))\n",
    "print(\"Error =\", err)\n",
    "time_plot(torch.abs(envelope[0, :] - envelope[1, :]).detach().cpu().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-variance",
   "metadata": {},
   "source": [
    "##### And here are the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#err.backward(retain_graph=True)\n",
    "#for p in adsr.torchparameters:\n",
    "#    print(adsr.torchparameters[p].data.grad)\n",
    "#    print(f\"{p} grad1={adsr.torchparameters[p].data.grad} grad2={adsr.torchparameters[p].data.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-hindu",
   "metadata": {},
   "source": [
    "We can also use an optimizer to match the parameters of the two ADSRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# optimizer = torch.optim.Adam(list(adsr2.parameters()), lr=0.01)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# time_plot(envelope.detach().cpu(), adsr.sample_rate, show=False)\n",
    "# time_plot(envelope2.detach().cpu(), adsr.sample_rate, show=False)\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     envelope = adsr(note_on_duration)\n",
    "#     envelope2 = adsr2(note_on_duration)\n",
    "#     err = torch.mean(torch.abs(envelope - envelope2))\n",
    "        \n",
    "#     if i % 10 == 0:\n",
    "#         ax.set_title(f\"Optimization Step {i} - Error: {err.item()}\")\n",
    "#         ax.lines[0].set_ydata(envelope.detach().cpu())\n",
    "#         ax.lines[1].set_ydata(envelope2.detach().cpu())\n",
    "#         fig.canvas.draw()\n",
    "    \n",
    "#     err.backward()\n",
    "#     optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-commercial",
   "metadata": {},
   "source": [
    "## Oscillators\n",
    "\n",
    "There are several types of oscillators and sound generators available. Oscillators that can be controlled by an external signal are called voltage-coltrolled oscillators (VCOs) in the analog world and we adpot a similar approach here; oscillators accept an input control signal and produce audio output. We have a simple sine oscilator:`TorchSineVCO`, a square/saw oscillator: `TorchSquareSawVCO`, and an FM oscillator: `TorchFmVCO`. There is also a white noise generator: `TorchNoise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Reset envelope\n",
    "adsr = TorchADSR(a, d, s, r, alpha).to(device)\n",
    "envelope = adsr.forward1D(note_on_duration)\n",
    "\n",
    "# Since the rest of the stuff is 1D, let's make the envelope batch_size 1\n",
    "envelope = envelope[0]\n",
    "\n",
    "# SineVCO test\n",
    "sine_vco = TorchSineVCO(midi_f0=T(12.0), mod_depth=T(50.0)).to(device)\n",
    "sine_out = sine_vco(envelope, phase=T(0.0))\n",
    "stft_plot(sine_out.detach().cpu().numpy())\n",
    "ipd.Audio(sine_out.detach().cpu().numpy(), rate=sine_vco.sample_rate.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-husband",
   "metadata": {},
   "source": [
    "SineVCO vs SineVCO with higher midi_f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SineVCO test\n",
    "midi_f0 = 12.0\n",
    "sine_vco2 = TorchSineVCO(midi_f0=T(30.0), mod_depth=T(50.0)).to(device)\n",
    "sine_out2 = sine_vco2(envelope, phase=T(0.0))\n",
    "stft_plot(sine_out2.detach().cpu().numpy())\n",
    "ipd.Audio(sine_out2.detach().cpu().numpy(), rate=sine_vco2.sample_rate.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-therapist",
   "metadata": {},
   "source": [
    "We can use auraloss instead of raw waveform loss. This is just to show that gradient computations occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = torch.mean(torch.abs(sine_out - sine_out2))\n",
    "print(\"Error =\", err)\n",
    "time_plot(torch.abs(sine_out - sine_out2).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#err.backward(retain_graph=True)\n",
    "#for p in sine_vco.torchparameters:\n",
    "#    print(f\"{p} grad1={sine_vco.torchparameters[p].grad.item()} grad2={sine_vco2.torchparameters[p].grad.item()}\")\n",
    "## Both SineVCOs use the sample envelope\n",
    "#for p in adsr.torchparameters:\n",
    "#    print(f\"{p} grad={adsr.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-sterling",
   "metadata": {},
   "source": [
    "### SquareSaw Oscillator\n",
    "\n",
    "Check this out, it's a square / saw oscillator. Use the shape parameter to\n",
    "interpolate between a square wave (shape = 0) and a sawtooth wave (shape = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.module import TorchSquareSawVCO\n",
    "\n",
    "square_saw1 = TorchSquareSawVCO(midi_f0=T(30.0), mod_depth=T(0.0), shape=T(0.0)).to(device)\n",
    "env1 = torch.zeros(square_saw1.buffer_size, device=device)\n",
    "\n",
    "square_saw_out1 = square_saw1(env1, phase=T(0.0))\n",
    "stft_plot(square_saw_out1.cpu().detach().numpy())\n",
    "ipd.Audio(square_saw_out1.cpu().detach().numpy(), rate=square_saw1.sample_rate.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SquareSawVCO test\n",
    "square_saw2 = TorchSquareSawVCO(midi_f0=T(30.0), mod_depth=T(0.0), shape=T(1.0)).to(device)\n",
    "env2 = torch.zeros(square_saw2.buffer_size, device=device)\n",
    "\n",
    "square_saw_out2 = square_saw2(env2, phase=T(0.0))\n",
    "stft_plot(square_saw_out2.cpu().detach().numpy())\n",
    "ipd.Audio(square_saw_out2.cpu().detach().numpy(), rate=square_saw2.sample_rate.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = torch.mean(torch.abs(square_saw_out2 - square_saw_out1))\n",
    "print(err)\n",
    "err.backward(retain_graph=True)\n",
    "for p in square_saw1.torchparameters:\n",
    "    print(f\"{p} grad1={square_saw1.torchparameters[p].grad.item()} grad2={square_saw2.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-institution",
   "metadata": {},
   "source": [
    "### VCA\n",
    "\n",
    "Notice that this sound is rather clicky. We'll add an envelope to the\n",
    "amplitude to smooth it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "vca = TorchVCA()\n",
    "test_output = vca(envelope, sine_out)\n",
    "\n",
    "time_plot(test_output.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-roller",
   "metadata": {},
   "source": [
    "### FM Synthesis\n",
    "\n",
    "What about FM? You bet. Use the `TorchFmVCO` class. It accepts any audio input.\n",
    "\n",
    "Just a note that, as in classic FM synthesis, you're dealing with a complex architecture of modulators. Each 'operator ' has its own pitch envelope, and amplitude envelope. The 'amplitude' envelope of an operator is really the *modulation depth* of the oscillator it operates on. So in the example below, we're using an ADSR to shape the depth of the *operator*, and this affects the modulation depth of the resultant signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.module import TorchFmVCO\n",
    "\n",
    "# FmVCO test\n",
    "midi_f0 = T(50.0)\n",
    "\n",
    "# Make steady-pitched sine (no pitch modulation).\n",
    "sine_operator = TorchSineVCO(midi_f0=midi_f0, mod_depth=T(0.0)).to(device)\n",
    "operator_out = sine_operator(envelope)\n",
    "\n",
    "# Shape the modulation depth.\n",
    "operator_out = vca(envelope, operator_out)\n",
    "\n",
    "# Feed into FM oscillator as modulator signal.\n",
    "fm_vco = TorchFmVCO(midi_f0=T(midi_f0), mod_depth=T(5.0)).to(device)\n",
    "fm_out = fm_vco(operator_out)\n",
    "\n",
    "stft_plot(fm_out.cpu().detach().numpy())\n",
    "ipd.Audio(fm_out.cpu().detach().numpy(), rate=fm_vco.sample_rate.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-minutes",
   "metadata": {},
   "source": [
    "### Noise\n",
    "\n",
    "The noise generator mixes white noise into a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we have buffer_size here?? isn't this a default?\n",
    "N = T(44100)\n",
    "\n",
    "env1 = torch.zeros(N)\n",
    "vco1 = TorchSineVCO(midi_f0=T(60), buffer_size=N)\n",
    "noise1 = TorchNoise(ratio=T(0.75), buffer_size=N)\n",
    "\n",
    "noisy_sine_1 = noise1(vco1(env1))\n",
    "\n",
    "env2 = torch.zeros(N)\n",
    "vco2 = TorchSineVCO(midi_f0=T(60), buffer_size=N)\n",
    "noise2 = TorchNoise(ratio=T(0.25), buffer_size=N)\n",
    "\n",
    "noisy_sine_2 = noise2(vco2(env2))\n",
    "\n",
    "stft_plot(noisy_sine_1.detach().numpy())\n",
    "ipd.display(ipd.Audio(noisy_sine_1.detach().numpy(), rate=vco1.sample_rate.item()))\n",
    "\n",
    "stft_plot(noisy_sine_2.detach().numpy())\n",
    "ipd.display(ipd.Audio(noisy_sine_2.detach().numpy(), rate=vco2.sample_rate.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error on the difference between the RMS level of the signals\n",
    "rms1 = torch.sqrt(torch.mean(noisy_sine_1 * noisy_sine_1))\n",
    "rms2 = torch.sqrt(torch.mean(noisy_sine_2 * noisy_sine_2))\n",
    "err = torch.abs(rms2 - rms1)\n",
    "print(err)\n",
    "\n",
    "err.backward(retain_graph=True)\n",
    "for p in noise1.torchparameters:\n",
    "    print(f\"{p} grad1={noise1.torchparameters[p].grad.item()} grad2={noise2.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(noise2.parameters()), lr=0.01)\n",
    "\n",
    "print(\"Parameters before optimization:\")\n",
    "print(list(noise1.parameters()))\n",
    "print(list(noise2.parameters()))\n",
    "\n",
    "error_hist = []\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    noisy_sine_1 = noise1(vco1(env1))\n",
    "    noisy_sine_2 = noise2(vco2(env2))\n",
    "\n",
    "    rms1 = torch.sqrt(torch.mean(noisy_sine_1 * noisy_sine_1))\n",
    "    rms2 = torch.sqrt(torch.mean(noisy_sine_2 * noisy_sine_2))\n",
    "    err = torch.abs(rms2 - rms1)\n",
    "\n",
    "    error_hist.append(err.item())\n",
    "    err.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "if isnotebook(): # pragma: no cover\n",
    "    plt.plot(error_hist)\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Optimization steps\")\n",
    "\n",
    "print(\"Parameters after optimization:\")\n",
    "print(list(noise1.parameters()))\n",
    "print(list(noise2.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-raise",
   "metadata": {},
   "source": [
    "## Drum Module\n",
    "\n",
    "Alternately, you can just use the Drum class that composes all these modules\n",
    "together automatically. The drum module comprises a set of envelopes and oscillators needed to create one-shot sounds similar to a drum hit generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drum1 = TorchDrum(\n",
    "    pitch_adsr=TorchADSR(0.25, 0.25, 0.25, 0.25, alpha=3),\n",
    "    amp_adsr=TorchADSR(0.25, 0.25, 0.25, 0.25),\n",
    "    vco_1=TorchSineVCO(midi_f0=69, mod_depth=12),\n",
    "    noise=TorchNoise(ratio=0.5),\n",
    "    note_on_duration=1.0,\n",
    ")\n",
    "\n",
    "drum_out1 = drum1()\n",
    "stft_plot(drum_out1.detach().numpy())\n",
    "ipd.Audio(drum_out1.detach().numpy(), rate=drum1.sample_rate.item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-romantic",
   "metadata": {},
   "source": [
    "Additionally, the Drum class can take two oscillators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drum2 = TorchDrum(\n",
    "    pitch_adsr=TorchADSR(0.1, 0.5, 0.0, 0.25, alpha=3),\n",
    "    amp_adsr=TorchADSR(0.1, 0.25, 0.25, 0.25),\n",
    "    vco_1=TorchSineVCO(midi_f0=40, mod_depth=12),\n",
    "    vco_2=TorchSquareSawVCO(midi_f0=40, mod_depth=12, shape=0.5),\n",
    "    noise=TorchNoise(ratio=0.01),\n",
    "    note_on_duration=1.0,\n",
    ")\n",
    "\n",
    "drum_out2 = drum2()\n",
    "stft_plot(drum_out2.detach().numpy())\n",
    "ipd.Audio(drum_out2.detach().numpy(), rate=drum2.sample_rate.item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-advertiser",
   "metadata": {},
   "source": [
    "Test gradients on entire drum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "err = torch.mean(torch.abs(drum_out1 - drum_out2))\n",
    "print(err)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-illustration",
   "metadata": {},
   "source": [
    "Print out the gradients for all the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "err.backward(retain_graph=True)\n",
    "\n",
    "for ((n1, p1), p2) in zip(drum1.named_parameters(), drum2.parameters()):\n",
    "    print(f\"{n1:40} Drum1: {p1.grad.item()} \\tDrum2: {p2.grad.item()}\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-picture",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-aspect",
   "metadata": {},
   "source": [
    "All synth modules and synth classes have named parameters which can be quered\n",
    "and updated. Let's look at the parameters for the Drum we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for n, p in drum1.named_parameters():\n",
    "    print(f\"{n:40} Normalized = {p:.2f} Human Range = {p.from_0to1():.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-junction",
   "metadata": {},
   "source": [
    "Parameters are passed into SynthModules during creation with an initial value and a parameter range. The parameter range is a human readable range of values, for example MIDI note numbers from 1-127 for a VCO. These values are stored in a normalized range between 0 and 1. Parameters can be accessed and set using either ranges with specific methods.\n",
    "\n",
    "Parameters of individual modules can be accessed in several ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Get the full ModuleParameter object by name from the module\n",
    "print(drum1.vco_1.get_parameter(\"pitch\"))\n",
    "\n",
    "# Access the value as a Tensor in the full value human range\n",
    "print(drum1.vco_1.p(\"pitch\"))\n",
    "\n",
    "# Access the value as a float in the range from 0 to 1\n",
    "print(drum1.vco_1.get_parameter_0to1(\"pitch\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-distributor",
   "metadata": {},
   "source": [
    "Parameters of individual modules can also be set using the human range or a normalized range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Set the vco pitch using the human range, which is MIDI note number\n",
    "drum1.vco_1.set_parameter(\"pitch\", 64)\n",
    "print(drum1.vco_1.p(\"pitch\"))\n",
    "\n",
    "# Set the vco pitch using a normalized range between 0 and 1\n",
    "drum1.vco_1.set_parameter_0to1(\"pitch\", 0.5433)\n",
    "print(drum1.vco_1.p(\"pitch\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-auction",
   "metadata": {},
   "source": [
    "## Random synths\n",
    "\n",
    "Let's generate some random synths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drum = TorchDrum(note_on_duration=1.0).to(device)\n",
    "for i in range(10):\n",
    "    drum.randomize()\n",
    "    drum_out = drum()\n",
    "    display(ipd.Audio(drum_out.cpu().detach().numpy(), rate=drum.sample_rate.item()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-plane",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.filter import TorchMovingAverage, FIRLowPass\n",
    "\n",
    "# Create some noise to filter\n",
    "duration = 2\n",
    "noise = torch.rand(2 * 44100, device=device) * 2 - 1\n",
    "stft_plot(noise.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-denver",
   "metadata": {},
   "source": [
    "**Moving Average Filter**\n",
    "\n",
    "A moving average filter is a simple finite impulse response (FIR) filter that calculates that value of a sample by taking the average of M input samples at a time. The filter_length defines how many samples M to include in the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ma_filter = TorchMovingAverage(filter_length=T(32.)).to(device)\n",
    "filtered = ma_filter(noise)\n",
    "\n",
    "stft_plot(filtered.cpu().detach().numpy())\n",
    "ipd.Audio(filtered.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second example with a longer filter -- notice that the filter length can be fractional\n",
    "ma_filter2 = TorchMovingAverage(filter_length=T(64.25)).to(device)\n",
    "filtered2 = ma_filter2(noise)\n",
    "\n",
    "stft_plot(filtered2.cpu().detach().numpy())\n",
    "ipd.Audio(filtered2.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-generic",
   "metadata": {},
   "source": [
    "Compute the error between the two examples and get the gradient for the filter length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft1 = torch.abs(torch.fft.fft(filtered))\n",
    "fft2 = torch.abs(torch.fft.fft(filtered2))\n",
    "\n",
    "err = torch.mean(torch.abs(fft1 - fft2))\n",
    "print(\"Error =\", err)\n",
    "\n",
    "err.backward(retain_graph=True)\n",
    "for p in ma_filter.torchparameters:\n",
    "    print(f\"{p} grad1={ma_filter.torchparameters[p].grad.item()} grad2={ma_filter2.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-newfoundland",
   "metadata": {},
   "source": [
    "**FIR Lowpass**\n",
    "\n",
    "The TorchFIR filter implements a low-pass filter by approximating the impulse response of an ideal lowpass filter, which is a windowed sinc function in the time domain. We can set the exact cut-off frequency for this filter, all frequencies above this point are attenuated. The quality of the approximation is determined by the length of the filter, choosing a larger filter length will result in a filter with a steeper slope at the cutoff and more attenuation of high frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fir1 = FIRLowPass(cutoff=T(1024), filter_length=T(128.0)).to(device)\n",
    "filtered1 = fir1(noise)\n",
    "\n",
    "stft_plot(filtered1.cpu().detach().numpy())\n",
    "ipd.Audio(filtered1.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second filter with a lower cutoff and a longer filter\n",
    "fir2 = FIRLowPass(cutoff=T(256.), filter_length=T(1024)).to(device)\n",
    "filtered2 = fir2(noise)\n",
    "\n",
    "stft_plot(filtered2.cpu().detach().numpy())\n",
    "ipd.Audio(filtered2.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-wedding",
   "metadata": {},
   "source": [
    "Compute the error between the two examples and check the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-guatemala",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fft1 = torch.abs(torch.fft.fft(filtered1))\n",
    "fft2 = torch.abs(torch.fft.fft(filtered2))\n",
    "err = torch.mean(torch.abs(fft1 - fft2))\n",
    "print(\"Error =\", err)\n",
    "\n",
    "err.backward(retain_graph=True)\n",
    "for p in fir1.torchparameters:\n",
    "    print(f\"{p} grad1={fir1.torchparameters[p].grad.item()} grad2={fir2.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-figure",
   "metadata": {},
   "source": [
    "#### IIR Filters\n",
    "\n",
    "A set of IIR filters using a SVF filter design approach are shown here, included filters are a lowpass, highpass, bandpass, and bandstop (or notch).\n",
    "\n",
    "IIR filters are really slow in Torch, so we're only testing with a shorter buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.filter import TorchLowPassSVF, TorchHighPassSVF, TorchBandPassSVF, TorchBandStopSVF\n",
    "import torch.fft\n",
    "\n",
    "# Noise for testing\n",
    "buffer = 4096\n",
    "noise = torch.tensor(np.random.random(buffer) * 2 - 1, device=device).float()\n",
    "stft_plot(noise.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-applicant",
   "metadata": {},
   "source": [
    "We'll create two lowpass filters with different cutoffs and filter resonance to compare. The second filter has higher resonance at the filter cutoff, this causes the filter to ring at that frequency. This can be seen in the spectrogram as a darker line at the cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpf1 = TorchLowPassSVF(cutoff=T(500), resonance=T(1.0), buffer_size=T(buffer)).to(device)\n",
    "filtered1 = lpf1(noise)\n",
    "\n",
    "stft_plot(filtered1.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpf2 = TorchLowPassSVF(cutoff=T(1000), resonance=T(10), buffer_size=T(buffer)).to(device)\n",
    "filtered2 = lpf2(noise)\n",
    "\n",
    "stft_plot(filtered2.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-cause",
   "metadata": {},
   "source": [
    "Error and gradients for the two lowpass filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum1 = torch.abs(torch.fft.fft(filtered1))\n",
    "spectrum2 = torch.abs(torch.fft.fft(filtered2))\n",
    "\n",
    "err = torch.mean(torch.abs(spectrum1 - spectrum2))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "err.backward(retain_graph=True)\n",
    "for p in lpf1.torchparameters:\n",
    "    print(f\"{p} grad1={lpf1.torchparameters[p].grad.item()} grad2={lpf2.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-deployment",
   "metadata": {},
   "source": [
    "Let's checkout some other SVF filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Highpass\n",
    "hpf = TorchHighPassSVF(cutoff=T(2048), buffer_size=T(buffer))\n",
    "filtered = hpf(noise)\n",
    "\n",
    "stft_plot(filtered.cpu().detach().numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-demonstration",
   "metadata": {},
   "source": [
    "We can also apply an envelope to the filter frequency. The mod_depth parameter determines how much effect the envelope will have on the cutoff. In this example a simple decay envelope is applied to the cutoff frequency, which has a base value of 20Hz, and has a duration of 100ms. The mod_depth is 10,000Hz, which means that as the envelope travels from 1 to 0, the cutoff will go from 10,020Hz down to 20Hz. The envelope is passed in as an extra argument to the call function on the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass with envelope\n",
    "env = TorchADSR(a=T([0]), d=T([0.1]), s=T([0.0]), r=T([0.0]), alpha=T([3.0]), buffer_size=T(buffer))(T([0.2]))\n",
    "bpf = TorchBandPassSVF(cutoff=T(20), resonance=T(30), mod_depth=T(10000), buffer_size=T(buffer))\n",
    "\n",
    "filtered = bpf(noise, env)\n",
    "# ParameterError: Audio buffer is not finite everywhere ????\n",
    "#stft_plot(filtered.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-handling",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Bandstop\n",
    "bsf = TorchBandStopSVF(cutoff=T(2000), resonance=T(0.05), buffer_size=T(buffer))\n",
    "filtered = bsf(noise)\n",
    "\n",
    "stft_plot(filtered.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-finnish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
