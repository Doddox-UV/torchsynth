{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchsynth examples\n",
    "\n",
    "We walk through basic functionality of `torchsynth` in this Jupyter notebook.\n",
    "\n",
    "Just note that all ipd.Audio play widgets normalize the audio.\n",
    "\n",
    "If you're in Colab, remember to set the runtime to GPU.\n",
    "and get the latest torchsynth:\n",
    "\n",
    "```\n",
    "!pip install git+https://github.com/turian/torchsynth.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iscolab():  # pragma: no cover\n",
    "    return \"google.colab\" in str(get_ipython())\n",
    "\n",
    "\n",
    "def isnotebook():  # pragma: no cover\n",
    "    try:\n",
    "        if iscolab():\n",
    "            return True\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == \"ZMQInteractiveShell\":\n",
    "            return True  # Jupyter notebook or qtconsole\n",
    "        elif shell == \"TerminalInteractiveShell\":\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False  # Probably standard Python interprete\n",
    "\n",
    "\n",
    "print(f\"isnotebook = {isnotebook()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if isnotebook():  # pragma: no cover\n",
    "    import IPython.display as ipd\n",
    "    import librosa\n",
    "    import librosa.display\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.core.display import display\n",
    "else:\n",
    "\n",
    "    class IPD:\n",
    "        def Audio(*args, **kwargs):\n",
    "            pass\n",
    "\n",
    "        def display(*args, **kwargs):\n",
    "            pass\n",
    "\n",
    "    ipd = IPD()\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import torch.fft\n",
    "import torch.tensor as T\n",
    "\n",
    "from torchsynth.default import DEFAULT_BUFFER_SIZE, DEFAULT_SAMPLE_RATE\n",
    "from torchsynth.globals import SynthGlobals\n",
    "from torchsynth.module import (\n",
    "    ADSR,\n",
    "    VCA,\n",
    "    Noise,\n",
    "    MonophonicKeyboard,\n",
    "    SineVCO,\n",
    "    TorchFmVCO,\n",
    ")\n",
    "from torchsynth.parameter import ModuleParameterRange\n",
    "\n",
    "# Determenistic seeds for replicable testing\n",
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run examples on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot(signal, sample_rate=DEFAULT_SAMPLE_RATE, show=True):\n",
    "    if isnotebook():  # pragma: no cover\n",
    "        t = np.linspace(0, len(signal) / sample_rate, len(signal), endpoint=False)\n",
    "        plt.plot(t, signal)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_plot(signal, sample_rate=DEFAULT_SAMPLE_RATE):\n",
    "    if isnotebook():  # pragma: no cover\n",
    "        X = librosa.stft(signal)\n",
    "        Xdb = librosa.amplitude_to_db(abs(X))\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        librosa.display.specshow(Xdb, sr=sample_rate, x_axis=\"time\", y_axis=\"log\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Globals\n",
    "# We'll generate 2 sounds at once, 4 seconds each\n",
    "synthglobals = SynthGlobals(\n",
    "    batch_size=T(2), sample_rate=T(44100), buffer_size=T(4 * 44100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a few examples, we'll only generate one sound\n",
    "synthglobals1 = SynthGlobals(\n",
    "    batch_size=T(1), sample_rate=T(44100), buffer_size=T(4 * 44100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a short one sound\n",
    "synthglobals1short = SynthGlobals(\n",
    "    batch_size=T(1), sample_rate=T(44100), buffer_size=T(4096)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Envelope\n",
    "Our module is based on an ADSR envelope, standing for \"attack, decay, sustain,\n",
    "release,\" which is specified by four values:\n",
    "\n",
    "- a: the attack time, in seconds; the time it takes for the signal to ramp\n",
    "     from 0 to 1.\n",
    "- d: the decay time, in seconds; the time to 'decay' from a peak of 1 to a\n",
    "     sustain level.\n",
    "- s: the sustain level; a value between 0 and 1 that the envelope holds during\n",
    "a sustained note (**not a time value**).\n",
    "- r: the release time, in seconds; the time it takes the signal to decay from\n",
    "     the sustain value to 0.\n",
    "\n",
    "Envelopes are used to modulate a variety of signals; usually one of pitch,\n",
    "amplitude, or filter cutoff frequency. In this notebook we will use the same\n",
    "envelope to modulate several different audio parameters.\n",
    "\n",
    "### A note about note-on, note-off behaviour\n",
    "\n",
    "By default, this envelope reacts as if it was triggered with midi, for example\n",
    "playing a keyboard. Each midi event has a beginning and end: note-on, when you\n",
    "press the key down; and note-off, when you release the key. `note_on_duration`\n",
    "is the amount of time that the key is depressed. During the note-on, the\n",
    "envelope moves through the attack and decay sections of the envelope. This\n",
    "leads to musically-intuitive, but programatically-counterintuitive behaviour.\n",
    "\n",
    "Assume attack is 0.5 seconds, and decay is 0.5 seconds. If a note is held for\n",
    "0.75 seconds, the envelope won't traverse through the entire attack-and-decay\n",
    "phase (specifically, it will execute the entire attack, and 0.25 seconds of\n",
    "the decay).\n",
    "\n",
    "If this is confusing, don't worry about it. ADSR's do a lot of work behind the\n",
    "scenes to make the playing experience feel natural. Alternately, you may\n",
    "specify one-shot mode (see below), which is more typical of drum machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis parameters.\n",
    "a = T([0.1, 0.2])\n",
    "d = T([0.1, 0.2])\n",
    "s = T([0.75, 0.8])\n",
    "r = T([0.5, 0.8])\n",
    "alpha = T([3.0, 4.0])\n",
    "note_on_duration = T([0.5, 1.5], device=device)\n",
    "\n",
    "# Envelope test\n",
    "adsr = ADSR(\n",
    "    attack=a, decay=d, sustain=s, release=r, alpha=alpha, synthglobals=synthglobals\n",
    ").to(device)\n",
    "envelope = adsr(note_on_duration)\n",
    "time_plot(envelope.clone().detach().cpu().T, adsr.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the l1 error between the two envelopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = torch.mean(torch.abs(envelope[0, :] - envelope[1, :]))\n",
    "print(\"Error =\", err)\n",
    "time_plot(torch.abs(envelope[0, :] - envelope[1, :]).detach().cpu().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And here are the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err.backward(retain_graph=True)\n",
    "# for p in adsr.torchparameters:\n",
    "#    print(adsr.torchparameters[p].data.grad)\n",
    "#    print(f\"{p} grad1={adsr.torchparameters[p].data.grad} grad2={adsr.torchparameters[p].data.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that module parameters are optional. If they are not provided,\n",
    "# they will be randomly initialized (like a typical neural network module)\n",
    "adsr = ADSR(synthglobals=synthglobals).to(device)\n",
    "envelope = adsr(note_on_duration)\n",
    "time_plot(envelope.clone().detach().cpu().T, adsr.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use an optimizer to match the parameters of the two ADSRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(list(adsr2.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "time_plot(envelope.detach().cpu(), adsr.sample_rate, show=False)\n",
    "time_plot(envelope2.detach().cpu(), adsr.sample_rate, show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    envelope = adsr(note_on_duration)\n",
    "    envelope2 = adsr2(note_on_duration)\n",
    "    err = torch.mean(torch.abs(envelope - envelope2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if i % 10 == 0:\n",
    "        ax.set_title(f\"Optimization Step {i} - Error: {err.item()}\")\n",
    "        ax.lines[0].set_ydata(envelope.detach().cpu())\n",
    "        ax.lines[1].set_ydata(envelope2.detach().cpu())\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "    err.backward()\n",
    "    optimizer.step()\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oscillators\n",
    "\n",
    "There are several types of oscillators and sound generators available. Oscillators that can be controlled by an external signal are called voltage-coltrolled oscillators (VCOs) in the analog world and we adpot a similar approach here; oscillators accept an input control signal and produce audio output. We have a simple sine oscilator:`SineVCO`, a square/saw oscillator: `SquareSawVCO`, and an FM oscillator: `TorchFmVCO`. There is also a white noise generator: `Noise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set up a Keyboard module\n",
    "keyboard = MonophonicKeyboard(\n",
    "    synthglobals, midi_f0=T([69.0, 50.0]), duration=note_on_duration\n",
    ").to(device)\n",
    "\n",
    "# Reset envelope\n",
    "adsr = ADSR(\n",
    "    attack=a, decay=d, sustain=s, release=r, alpha=alpha, synthglobals=synthglobals\n",
    ").to(device)\n",
    "\n",
    "# Trigger the keyboard, which returns a midi_f0 and note duration\n",
    "midi_f0, duration = keyboard()\n",
    "\n",
    "envelope = adsr(duration)\n",
    "\n",
    "# SineVCO test\n",
    "sine_vco = SineVCO(\n",
    "    tuning=T([0.0, 0.0]), mod_depth=T([-12.0, 12.0]), synthglobals=synthglobals\n",
    ").to(device)\n",
    "sine_out = sine_vco(midi_f0, envelope)\n",
    "\n",
    "stft_plot(sine_out[0].detach().cpu().numpy())\n",
    "ipd.display(\n",
    "    ipd.Audio(sine_out[0].detach().cpu().numpy(), rate=sine_vco.sample_rate.item())\n",
    ")\n",
    "stft_plot(sine_out[1].detach().cpu().numpy())\n",
    "ipd.display(\n",
    "    ipd.Audio(sine_out[1].detach().cpu().numpy(), rate=sine_vco.sample_rate.item())\n",
    ")\n",
    "\n",
    "# We can use auraloss instead of raw waveform loss. This is just\n",
    "# to show that gradient computations occur\n",
    "err = torch.mean(torch.abs(sine_out[0] - sine_out[1]))\n",
    "print(\"Error =\", err)\n",
    "time_plot(torch.abs(sine_out[0] - sine_out[1]).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err.backward(retain_graph=True)\n",
    "# for p in sine_vco.torchparameters:\n",
    "#    print(f\"{p} grad1={sine_vco.torchparameters[p].grad.item()} grad2={sine_vco2.torchparameters[p].grad.item()}\")\n",
    "## Both SineVCOs use the sample envelope\n",
    "# for p in adsr.torchparameters:\n",
    "#    print(f\"{p} grad={adsr.torchparameters[p].grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SquareSaw Oscillator\n",
    "\n",
    "Check this out, it's a square / saw oscillator. Use the shape parameter to\n",
    "interpolate between a square wave (shape = 0) and a sawtooth wave (shape = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.module import SquareSawVCO\n",
    "\n",
    "keyboard = MonophonicKeyboard(synthglobals, midi_f0=T([30.0, 30.0])).to(device)\n",
    "\n",
    "square_saw = SquareSawVCO(\n",
    "    tuning=T([0.0, 0.0]),\n",
    "    mod_depth=T([0.0, 0.0]),\n",
    "    shape=T([0.0, 1.0]),\n",
    "    synthglobals=synthglobals,\n",
    ").to(device)\n",
    "env2 = torch.zeros([2, square_saw.buffer_size], device=device)\n",
    "\n",
    "square_saw_out = square_saw(keyboard.p(\"midi_f0\"), env2)\n",
    "stft_plot(square_saw_out[0].cpu().detach().numpy())\n",
    "ipd.display(\n",
    "    ipd.Audio(\n",
    "        square_saw_out[0].cpu().detach().numpy(), rate=square_saw.sample_rate.item()\n",
    "    )\n",
    ")\n",
    "stft_plot(square_saw_out[1].cpu().detach().numpy())\n",
    "ipd.display(\n",
    "    ipd.Audio(\n",
    "        square_saw_out[1].cpu().detach().numpy(), rate=square_saw.sample_rate.item()\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "err = torch.mean(torch.abs(square_saw_out[0] - square_saw_out[1]))\n",
    "print(err)\n",
    "# err.backward(retain_graph=True)\n",
    "# for p in square_saw.torchparameters:\n",
    "#    print(f\"{p} grad1={square_saw.torchparameters[p][0].grad.item()} grad2={square_saw.torchparameters[p][1].grad.item()}\")\n",
    "\n",
    "# ### VCA\n",
    "#\n",
    "# Notice that this sound is rather clicky. We'll add an envelope to the\n",
    "# amplitude to smooth it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vca = VCA(synthglobals)\n",
    "test_output = vca(envelope, sine_out)\n",
    "\n",
    "time_plot(test_output[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM Synthesis\n",
    "\n",
    "What about FM? You bet. Use the `TorchFmVCO` class. It accepts any audio input.\n",
    "\n",
    "Just a note that, as in classic FM synthesis, you're dealing with a complex architecture of modulators. Each 'operator ' has its own pitch envelope, and amplitude envelope. The 'amplitude' envelope of an operator is really the *modulation depth* of the oscillator it operates on. So in the example below, we're using an ADSR to shape the depth of the *operator*, and this affects the modulation depth of the resultant signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FmVCO test\n",
    "\n",
    "keyboard = MonophonicKeyboard(synthglobals, midi_f0=T([50.0, 50.0])).to(device)\n",
    "\n",
    "# Make steady-pitched sine (no pitch modulation).\n",
    "sine_operator = SineVCO(\n",
    "    tuning=T([0.0, 0.0]), mod_depth=T([0.0, 5.0]), synthglobals=synthglobals\n",
    ").to(device)\n",
    "operator_out = sine_operator(keyboard.p(\"midi_f0\"), envelope)\n",
    "\n",
    "# Shape the modulation depth.\n",
    "operator_out = vca(envelope, operator_out)\n",
    "\n",
    "# Feed into FM oscillator as modulator signal.\n",
    "fm_vco = TorchFmVCO(\n",
    "    tuning=T([0.0, 0.0]), mod_depth=T([2.0, 5.0]), synthglobals=synthglobals\n",
    ").to(device)\n",
    "fm_out = fm_vco(keyboard.p(\"midi_f0\"), operator_out)\n",
    "\n",
    "stft_plot(fm_out[0].cpu().detach().numpy())\n",
    "ipd.display(ipd.Audio(fm_out[0].cpu().detach().numpy(), rate=fm_vco.sample_rate.item()))\n",
    "\n",
    "stft_plot(fm_out[1].cpu().detach().numpy())\n",
    "ipd.display(ipd.Audio(fm_out[1].cpu().detach().numpy(), rate=fm_vco.sample_rate.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise\n",
    "\n",
    "The noise generator mixes white noise into a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = torch.zeros([2, DEFAULT_BUFFER_SIZE], device=device)\n",
    "vco = SineVCO(\n",
    "    tuning=T([0.0, 0.0]), mod_depth=T([0.0, 5.0]), synthglobals=synthglobals\n",
    ").to(device)\n",
    "noise = Noise(ratio=T([0.75, 0.25]), synthglobals=synthglobals).to(device)\n",
    "\n",
    "noisy_sine = noise(vco(keyboard.p(\"midi_f0\"), env))\n",
    "\n",
    "stft_plot(noisy_sine[0].detach().cpu().numpy())\n",
    "ipd.display(\n",
    "    ipd.Audio(noisy_sine[0].detach().cpu().numpy(), rate=vco.sample_rate.item())\n",
    ")\n",
    "\n",
    "stft_plot(noisy_sine[1].detach().cpu().numpy())\n",
    "ipd.display(\n",
    "    ipd.Audio(noisy_sine[1].detach().cpu().numpy(), rate=vco.sample_rate.item())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error on the difference between the RMS level of the signals\n",
    "rms0 = torch.sqrt(torch.mean(noisy_sine[0] * noisy_sine[0]))\n",
    "rms1 = torch.sqrt(torch.mean(noisy_sine[1] * noisy_sine[1]))\n",
    "err = torch.abs(rms1 - rms0)\n",
    "print(err)\n",
    "\n",
    "# err.backward(retain_graph=True)\n",
    "# for p in noise.torchparameters:\n",
    "#    print(f\"{p} grad1={noise.torchparameters[p][0].grad.item()} grad2={noise.torchparameters[p][1].grad.item()}\")\n",
    "\n",
    "\"\"\"\n",
    "# +\n",
    "optimizer = torch.optim.Adam(list(noise2.parameters()), lr=0.01)\n",
    "\n",
    "print(\"Parameters before optimization:\")\n",
    "print(list(noise.parameters()))\n",
    "\n",
    "error_hist = []\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    noisy_sine = noise(vco(env))\n",
    "    rms0 = torch.sqrt(torch.mean(noisy_sine[0] * noisy_sine[0]))\n",
    "    rms1 = torch.sqrt(torch.mean(noisy_sine[1] * noisy_sine[1]))\n",
    "    err = torch.abs(rms1 - rms0)\n",
    "\n",
    "    error_hist.append(err.item())\n",
    "    err.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "if isnotebook():  # pragma: no cover\n",
    "    plt.plot(error_hist)\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Optimization steps\")\n",
    "\n",
    "print(\"Parameters after optimization:\")\n",
    "print(list(noise.parameters()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modulation\n",
    "\n",
    "Besides envelopes, LFOs can be used to modulate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.module import SineLFO\n",
    "\n",
    "lfo = SineLFO(synthglobals)\n",
    "out = lfo()\n",
    "\n",
    "if isnotebook():\n",
    "    plt.plot(out[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice Module\n",
    "\n",
    "Alternately, you can just use the Voice class that composes all these modules\n",
    "together automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.synth import Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice1 = Voice(synthglobals=synthglobals1).to(device)\n",
    "voice1.set_parameters(\n",
    "    {\n",
    "        (\"keyboard\", \"midi_f0\"): T([69.0]),\n",
    "        (\"keyboard\", \"duration\"): T([1.0]),\n",
    "        (\"pitch_adsr\", \"attack\"): T([0.25]),\n",
    "        (\"pitch_adsr\", \"decay\"): T([0.25]),\n",
    "        (\"pitch_adsr\", \"sustain\"): T([0.25]),\n",
    "        (\"pitch_adsr\", \"release\"): T([0.25]),\n",
    "        (\"pitch_adsr\", \"alpha\"): T([3.0]),\n",
    "        (\"amp_adsr\", \"attack\"): T([0.25]),\n",
    "        (\"amp_adsr\", \"decay\"): T([0.25]),\n",
    "        (\"amp_adsr\", \"sustain\"): T([0.25]),\n",
    "        (\"amp_adsr\", \"release\"): T([0.25]),\n",
    "        (\"amp_adsr\", \"alpha\"): T([3.0]),\n",
    "        (\"vco_1\", \"tuning\"): T([0.0]),\n",
    "        (\"vco_1\", \"mod_depth\"): T([12.0]),\n",
    "        (\"vco_ratio\", \"ratio\"): T([0.0]),\n",
    "        (\"noise\", \"ratio\"): T([0.05]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "voice_out1 = voice1()\n",
    "stft_plot(voice_out1.cpu().view(-1).detach().numpy())\n",
    "ipd.Audio(voice_out1.cpu().detach().numpy(), rate=voice1.sample_rate.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Additionally, the Voice class can take two oscillators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "voice2 = Voice(synthglobals=synthglobals1).to(device)\n",
    "voice2.set_parameters(\n",
    "    {\n",
    "        (\"keyboard\", \"midi_f0\"): T([40.0]),\n",
    "        (\"keyboard\", \"duration\"): T([3.0]),\n",
    "        (\"pitch_adsr\", \"attack\"): T([0.0]),\n",
    "        (\"pitch_adsr\", \"decay\"): T([2.0]),\n",
    "        (\"pitch_adsr\", \"sustain\"): T([0.0]),\n",
    "        (\"pitch_adsr\", \"release\"): T([0.0]),\n",
    "        (\"pitch_adsr\", \"alpha\"): T([2.0]),\n",
    "        (\"amp_adsr\", \"attack\"): T([0.1]),\n",
    "        (\"amp_adsr\", \"decay\"): T([0.25]),\n",
    "        (\"amp_adsr\", \"sustain\"): T([1.0]),\n",
    "        (\"amp_adsr\", \"release\"): T([0.5]),\n",
    "        (\"amp_adsr\", \"alpha\"): T([3.0]),\n",
    "        (\"vco_1\", \"tuning\"): T([19.0]),\n",
    "        (\"vco_1\", \"mod_depth\"): T([24.0]),\n",
    "        (\"vco_2\", \"tuning\"): T([0.0]),\n",
    "        (\"vco_2\", \"mod_depth\"): T([12.0]),\n",
    "        (\"vco_2\", \"shape\"): T([1.0]),\n",
    "        (\"vco_ratio\", \"ratio\"): T([0.5]),\n",
    "        (\"noise\", \"ratio\"): T([0.0]),\n",
    "    }\n",
    ")\n",
    "\n",
    "voice_out2 = voice2()\n",
    "stft_plot(voice_out2.cpu().view(-1).detach().numpy())\n",
    "ipd.Audio(voice_out2.cpu().detach().numpy(), rate=voice2.sample_rate.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test gradients on entire voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = torch.mean(torch.abs(voice_out1 - voice_out2))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random synths\n",
    "\n",
    "Let's generate some random synths in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthglobals16 = SynthGlobals(\n",
    "    batch_size=T(16), sample_rate=T(44100), buffer_size=T(4 * 44100)\n",
    ")\n",
    "voice = Voice(synthglobals=synthglobals16).to(device)\n",
    "voice_out = voice()\n",
    "for i in range(synthglobals16.batch_size):\n",
    "    stft_plot(voice_out[i].cpu().view(-1).detach().numpy())\n",
    "    ipd.display(\n",
    "        ipd.Audio(voice_out[i].cpu().detach().numpy(), rate=voice.sample_rate.item())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be set and frozen before randomization as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_f0 = torch.full((voice.batch_size.item(),), 42.0)\n",
    "duration = torch.full((voice.batch_size.item(),), 3.0)\n",
    "tuning = torch.full((voice.batch_size.item(),), 0.0)\n",
    "\n",
    "voice.unfreeze_all_parameters()\n",
    "voice.set_parameters(\n",
    "    {\n",
    "        (\"keyboard\", \"midi_f0\"): midi_f0,\n",
    "        (\"keyboard\", \"duration\"): duration,\n",
    "        (\"vco_1\", \"tuning\"): tuning,\n",
    "        (\"vco_2\", \"tuning\"): tuning,\n",
    "    },\n",
    "    freeze=True,\n",
    ")\n",
    "\n",
    "voice_out = voice()\n",
    "for i in range(synthglobals16.batch_size):\n",
    "    stft_plot(voice_out[i].cpu().view(-1).detach().numpy())\n",
    "    ipd.display(\n",
    "        ipd.Audio(voice_out[i].cpu().detach().numpy(), rate=voice.sample_rate.item())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All synth modules and synth classes have named parameters which can be quered\n",
    "and updated. Let's look at the parameters for the Voice we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in voice1.named_parameters():\n",
    "    print(f\"{n:40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are passed into SynthModules during creation with an initial value and a parameter range. The parameter range is a human readable range of values, for example MIDI note numbers from 1-127 for a VCO. These values are stored in a normalized range between 0 and 1. Parameters can be accessed and set using either ranges with specific methods.\n",
    "\n",
    "Parameters of individual modules can be accessed in several ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full ModuleParameter object by name from the module\n",
    "print(voice1.vco_1.get_parameter(\"tuning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the value as a Tensor in the full value human range\n",
    "print(voice1.vco_1.p(\"tuning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the value as a float in the range from 0 to 1\n",
    "print(voice1.vco_1.get_parameter_0to1(\"tuning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of individual modules can also be set using the human range or a normalized range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the vco pitch using the human range, which is MIDI note number\n",
    "voice1.vco_1.set_parameter(\"tuning\", T([64]))\n",
    "print(voice1.vco_1.p(\"tuning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the vco pitch using a normalized range between 0 and 1\n",
    "voice1.vco_1.set_parameter_0to1(\"tuning\", T([0.5433]))\n",
    "print(voice1.vco_1.p(\"tuning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Ranges\n",
    "\n",
    "Conversion between [0,1] range and a human range is handled by `ModuleParameterRange`. The conversion from [0,1] can be shaped by specifying a curve. Curve values less than 1 put more emphasis on lower values in the human range and curve values greater than 1 put more emphasis on larger values in the human range. A curve of 1 is a linear relationship between the two ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ModuleParameterRange with scaling of a range from 0-127\n",
    "param_range_exp = ModuleParameterRange(0.0, 127.0, curve=0.5)\n",
    "param_range_lin = ModuleParameterRange(0.0, 127.0, curve=1.0)\n",
    "param_range_log = ModuleParameterRange(0.0, 127.0, curve=2.0)\n",
    "\n",
    "# Linearly spaced values from 0.0 1.0\n",
    "param_values = torch.linspace(0.0, 1.0, 100)\n",
    "\n",
    "if isnotebook():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "    axes[0].plot(param_values, param_range_exp.from_0to1(param_values))\n",
    "    axes[0].set_title(\"Exponential Scaling\")\n",
    "\n",
    "    axes[1].plot(param_values, param_range_lin.from_0to1(param_values))\n",
    "    axes[1].set_title(\"Linear Scaling\")\n",
    "\n",
    "    axes[2].plot(param_values, param_range_log.from_0to1(param_values))\n",
    "    axes[2].set_title(\"Logarithmic Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleParameterRange with symmetric scaling of a range from -127 to 127\n",
    "param_range_exp = ModuleParameterRange(-127.0, 127.0, curve=0.5, symmetric=True)\n",
    "param_range_log = ModuleParameterRange(-127.0, 127.0, curve=2.0, symmetric=True)\n",
    "\n",
    "# Linearly spaced values from 0.0 1.0\n",
    "param_values = torch.linspace(0.0, 1.0, 100)\n",
    "\n",
    "if isnotebook():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "    axes[0].plot(param_values, param_range_exp.from_0to1(param_values))\n",
    "    axes[0].set_title(\"Exponential Scaling\")\n",
    "\n",
    "    axes[1].plot(param_values, param_range_log.from_0to1(param_values))\n",
    "    axes[1].set_title(\"Logarithmic Scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.filter import FIRLowPass, MovingAverage\n",
    "\n",
    "# GPU not working for filters yet\n",
    "device = \"cpu\"\n",
    "\n",
    "# Create some noise to filter\n",
    "duration = 2\n",
    "noise = torch.rand(2 * 44100, device=device) * 2 - 1\n",
    "stft_plot(noise.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving Average Filter**\n",
    "\n",
    "A moving average filter is a simple finite impulse response (FIR) filter that calculates that value of a sample by taking the average of M input samples at a time. The filter_length defines how many samples M to include in the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_filter = MovingAverage(filter_length=T(32.0)).to(device)\n",
    "filtered = ma_filter(noise)\n",
    "\n",
    "stft_plot(filtered.cpu().detach().numpy())\n",
    "ipd.Audio(filtered.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second example with a longer filter -- notice that the filter length can be fractional\n",
    "ma_filter2 = MovingAverage(filter_length=T(64.25)).to(device)\n",
    "filtered2 = ma_filter2(noise)\n",
    "\n",
    "stft_plot(filtered2.cpu().detach().numpy())\n",
    "ipd.Audio(filtered2.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the error between the two examples and get the gradient for the filter length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft1 = torch.abs(torch.fft.fft(filtered))\n",
    "fft2 = torch.abs(torch.fft.fft(filtered2))\n",
    "\n",
    "err = torch.mean(torch.abs(fft1 - fft2))\n",
    "print(\"Error =\", err)\n",
    "\n",
    "err.backward(retain_graph=True)\n",
    "for p in ma_filter.torchparameters:\n",
    "    print(\n",
    "        f\"{p} grad1={ma_filter.torchparameters[p].grad.item()} grad2={ma_filter2.torchparameters[p].grad.item()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIR Lowpass**\n",
    "\n",
    "The TorchFIR filter implements a low-pass filter by approximating the impulse response of an ideal lowpass filter, which is a windowed sinc function in the time domain. We can set the exact cut-off frequency for this filter, all frequencies above this point are attenuated. The quality of the approximation is determined by the length of the filter, choosing a larger filter length will result in a filter with a steeper slope at the cutoff and more attenuation of high frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fir1 = FIRLowPass(cutoff=T(1024), filter_length=T(128.0)).to(device)\n",
    "filtered1 = fir1(noise)\n",
    "\n",
    "stft_plot(filtered1.cpu().detach().numpy())\n",
    "ipd.Audio(filtered1.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second filter with a lower cutoff and a longer filter\n",
    "fir2 = FIRLowPass(cutoff=T(256.0), filter_length=T(1024)).to(device)\n",
    "filtered2 = fir2(noise)\n",
    "\n",
    "stft_plot(filtered2.cpu().detach().numpy())\n",
    "ipd.Audio(filtered2.cpu().detach().numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the error between the two examples and check the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fft1 = torch.abs(torch.fft.fft(filtered1))\n",
    "fft2 = torch.abs(torch.fft.fft(filtered2))\n",
    "err = torch.mean(torch.abs(fft1 - fft2))\n",
    "print(\"Error =\", err)\n",
    "\n",
    "err.backward(retain_graph=True)\n",
    "for p in fir1.torchparameters:\n",
    "    print(\n",
    "        f\"{p} grad1={fir1.torchparameters[p].grad.item()} grad2={fir2.torchparameters[p].grad.item()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IIR Filters\n",
    "\n",
    "A set of IIR filters using a SVF filter design approach are shown here, included filters are a lowpass, highpass, bandpass, and bandstop (or notch).\n",
    "\n",
    "IIR filters are really slow in Torch, so we're only testing with a shorter buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsynth.filter import (\n",
    "    TorchBandPassSVF,\n",
    "    TorchBandStopSVF,\n",
    "    TorchHighPassSVF,\n",
    "    TorchLowPassSVF,\n",
    ")\n",
    "\n",
    "# Noise for testing\n",
    "noise = torch.tensor(\n",
    "    np.random.random(synthglobals1short.buffer_size) * 2 - 1, device=device\n",
    ").float()\n",
    "stft_plot(noise.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two lowpass filters with different cutoffs and filter resonance to compare. The second filter has higher resonance at the filter cutoff, this causes the filter to ring at that frequency. This can be seen in the spectrogram as a darker line at the cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpf1 = TorchLowPassSVF(\n",
    "    cutoff=T(500), resonance=T(1.0), buffer_size=T(synthglobals1short.buffer_size)\n",
    ").to(device)\n",
    "filtered1 = lpf1(noise)\n",
    "\n",
    "stft_plot(filtered1.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpf2 = TorchLowPassSVF(\n",
    "    cutoff=T(1000), resonance=T(10), buffer_size=T(synthglobals1short.buffer_size)\n",
    ").to(device)\n",
    "filtered2 = lpf2(noise)\n",
    "\n",
    "stft_plot(filtered2.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error and gradients for the two lowpass filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum1 = torch.abs(torch.fft.fft(filtered1))\n",
    "spectrum2 = torch.abs(torch.fft.fft(filtered2))\n",
    "\n",
    "err = torch.mean(torch.abs(spectrum1 - spectrum2))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err.backward(retain_graph=True)\n",
    "for p in lpf1.torchparameters:\n",
    "    print(\n",
    "        f\"{p} grad1={lpf1.torchparameters[p].grad.item()} grad2={lpf2.torchparameters[p].grad.item()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout some other SVF filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Highpass\n",
    "hpf = TorchHighPassSVF(cutoff=T(2048), buffer_size=T(synthglobals1.buffer_size))\n",
    "filtered = hpf(noise)\n",
    "\n",
    "stft_plot(filtered.cpu().detach().numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply an envelope to the filter frequency. The mod_depth parameter determines how much effect the envelope will have on the cutoff. In this example a simple decay envelope is applied to the cutoff frequency, which has a base value of 20Hz, and has a duration of 100ms. The mod_depth is 10,000Hz, which means that as the envelope travels from 1 to 0, the cutoff will go from 10,020Hz down to 20Hz. The envelope is passed in as an extra argument to the call function on the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass with envelope\n",
    "env = ADSR(\n",
    "    attack=T([0]),\n",
    "    decay=T([0.1]),\n",
    "    sustain=T([0.0]),\n",
    "    release=T([0.0]),\n",
    "    alpha=T([3.0]),\n",
    "    synthglobals=synthglobals1short,\n",
    ")(T([0.2]))\n",
    "bpf = TorchBandPassSVF(\n",
    "    cutoff=T(20),\n",
    "    resonance=T(30),\n",
    "    mod_depth=T(10000),\n",
    "    buffer_size=T(synthglobals1short.buffer_size),\n",
    ")\n",
    "\n",
    "filtered = bpf(noise, env[0])\n",
    "# ParameterError: Audio buffer is not finite everywhere ????\n",
    "# stft_plot(filtered.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandstop\n",
    "bsf = TorchBandStopSVF(\n",
    "    cutoff=T(2000), resonance=T(0.05), buffer_size=T(synthglobals1short.buffer_size)\n",
    ")\n",
    "filtered = bsf(noise)\n",
    "\n",
    "stft_plot(filtered.cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
