# -*- coding: utf-8 -*-
"""lightningsynth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/turian/torchsynth/blob/lightning-synth/examples/lightningsynth.ipynb

# lightningsynth

Profiling for our synth on GPUs

Make sure you are on GPU runtime

If this hasn't been merged to master yet, run:
```
!pip uninstall -y torchsynth
!pip install git+https://github.com/turian/torchsynth.git@lightning-synth
```
"""

#!pip uninstall -y torchsynth
#!pip install git+https://github.com/turian/torchsynth.git@lightning-synth

#!pip install torchvision

import torch
import torch.tensor as T
from tqdm.auto import tqdm

import torch

# import torchvision.models as models
import torch.autograd.profiler as profiler
import pytorch_lightning as pl

from torchsynth.globals import SynthGlobals
from torchsynth.synth import Voice
import torchsynth.module

gpus = torch.cuda.device_count()
print("Usings %d gpus" % gpus)

# Note this is the batch size for our synth!
# Not the batch size of the datasets
BATCH_SIZE = 1024

import multiprocessing

ncores = multiprocessing.cpu_count()
print(f"Using ncores {ncores} for generating batch numbers (low CPU usage)")


class batch_idx_dataset(torch.utils.data.Dataset):
    def __init__(self, num_batches):
        self.num_batches = num_batches

    def __getitem__(self, idx):
        return idx

    def __len__(self):
        return self.num_batches


synth1M = batch_idx_dataset(1024 * 1024 // BATCH_SIZE)

test_dataloader = torch.utils.data.DataLoader(synth1M, num_workers=0, batch_size=1)

synthglobals = SynthGlobals(batch_size=T(256))
voice = Voice(synthglobals)

# TODO: Change precision?
# specifies all available GPUs (if only one GPU is not occupied, uses one gpu)
# Use deterministic?
trainer = pl.Trainer(precision=16, gpus=-1, auto_select_gpus=True, accelerator='ddp', deterministic=True, max_epochs=0)
trainer.test(voice, test_dataloaders=test_dataloader)
